{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Classfication model with neural networks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset X containing (x1, x2) coordinates in the columns:\n",
      "[[1 1 0 1 0 0 0 1 1 1 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0]\n",
      " [1 0 0 1 0 1 0 0 0 1 1 1 0 1 1 0 1 0 1 0 1 1 1 1 0 1 0 1 1 0]]\n",
      "Training dataset Y containing labels of two classes (0: blue, 1: red)\n",
      "[[0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 1 1 0 1 0 0 1 0]]\n",
      "The shape of X is: (2, 30)\n",
      "The shape of Y is: (1, 30)\n",
      "Number of training examples = 30\n"
     ]
    }
   ],
   "source": [
    "m = 30\n",
    "\n",
    "X = np.random.randint(0, 2, (2, m))\n",
    "Y = np.logical_and(X[0] == 0, X[1] == 1).astype(int).reshape((1, m))\n",
    "\n",
    "print('Training dataset X containing (x1, x2) coordinates in the columns:')\n",
    "print(X)\n",
    "print('Training dataset Y containing labels of two classes (0: blue, 1: red)')\n",
    "print(Y)\n",
    "\n",
    "print ('The shape of X is: ' + str(X.shape))\n",
    "print ('The shape of Y is: ' + str(Y.shape))\n",
    "print ('Number of training examples = ' + str(X.shape[1]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Defining Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid(-2) = 0.11920292202211755\n",
      "sigmoid(0) = 0.5\n",
      "sigmoid(3.5) = 0.9706877692486436\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1 + np.exp(-z))\n",
    "\n",
    "print(\"sigmoid(-2) = \" + str(sigmoid(-2)))\n",
    "print(\"sigmoid(0) = \" + str(sigmoid(0)))\n",
    "print(\"sigmoid(3.5) = \" + str(sigmoid(3.5)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Implementing Neural Network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 - Defining the Neural Network Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the input layer is: n_x = 2\n",
      "The size of the output layer is: n_y = 1\n"
     ]
    }
   ],
   "source": [
    "def layer_sizes(X, Y):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    X -- input dataset\n",
    "    Y -- labels\n",
    "    \n",
    "    Returns:\n",
    "    n_x -- size of input layer\n",
    "    n_y -- size of output layer\n",
    "    \"\"\"\n",
    "    \n",
    "    n_x = X.shape[0]\n",
    "    n_y = Y.shape[0]\n",
    "    \n",
    "    return (n_x, n_y)\n",
    "\n",
    "(n_x, n_y) = layer_sizes(X, Y)\n",
    "print(\"The size of the input layer is: n_x = \" + str(n_x))\n",
    "print(\"The size of the output layer is: n_y = \" + str(n_y))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Initializing Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W = [[0.00869127 0.00436173]]\n",
      "b = [[0.]]\n"
     ]
    }
   ],
   "source": [
    "def initialize_parameters(n_x, n_y):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        n_x -- size of input layer\n",
    "        n_y -- size of output layer\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dict containing:\n",
    "                        W -- weights matrix for neural network\n",
    "                        b -- bias value set as vector\n",
    "    \"\"\"\n",
    "    \n",
    "    W = np.random.rand(n_y, n_x) * 0.01\n",
    "    b = np.zeros((n_y, 1))\n",
    "    \n",
    "    parameters = {\"W\" : W,\n",
    "                  \"b\" : b}\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "parameters = initialize_parameters(n_x, n_y)\n",
    "print(\"W = \" + str(parameters[\"W\"]))\n",
    "print(\"b = \" + str(parameters[\"b\"]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Forward Propagation And Backward Propagation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing `forward_propagation()` following the equation:\n",
    "\\begin{align}\n",
    "Z &=  W X + b,\\\\\n",
    "A &= \\sigma\\left(Z\\right).\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output vector A: [[0.50326321 0.5021728  0.5        0.50326321 0.5        0.50109043\n",
      "  0.5        0.5021728  0.5021728  0.50326321 0.50326321 0.50326321\n",
      "  0.5        0.50109043 0.50326321 0.5        0.50109043 0.5\n",
      "  0.50109043 0.5        0.50109043 0.50109043 0.50109043 0.50109043\n",
      "  0.5021728  0.50109043 0.5021728  0.50326321 0.50109043 0.5       ]]\n"
     ]
    }
   ],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        X -- input data\n",
    "        parameters -- python dict containing:\n",
    "                        W -- weights matrix for neural network\n",
    "                        b -- bias value set as vector\n",
    "    Returns:\n",
    "        A -- The output\n",
    "    \"\"\"\n",
    "    W = parameters[\"W\"]\n",
    "    b = parameters[\"b\"]\n",
    "    \n",
    "    # Forward Propagation to calculate Z\n",
    "    Z = np.matmul(W, X) + b\n",
    "    A = sigmoid(Z)\n",
    "    \n",
    "    return A\n",
    "\n",
    "A = forward_propagation(X, parameters)\n",
    "\n",
    "print(\"Output vector A:\", A)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing cost using `computing_cost()` to update parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost = 0.6946746844429882\n"
     ]
    }
   ],
   "source": [
    "def compute_cost(A, Y):\n",
    "    # Computes the log loss\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        A -- The output of the neural network\n",
    "        Y -- Orginal output values to the input\n",
    "        \n",
    "    Returns:\n",
    "    cost -- log loss\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "    \n",
    "    # Computing Cost\n",
    "    logprobs = - np.multiply(np.log(A), Y) - np.multiply(np.log(1 - A), 1 - Y)\n",
    "    cost = 1/m * np.sum(logprobs)\n",
    "    \n",
    "    return cost\n",
    "\n",
    "print(\"cost = \" + str(compute_cost(A, Y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dW = [[ 0.20112355 -0.04887511]]\n",
      "db = [[0.16815369]]\n"
     ]
    }
   ],
   "source": [
    "def backward_propagation(A, X, Y):\n",
    "    \"\"\"Implements the backward propagation, calculating gradients\n",
    "\n",
    "    Args:\n",
    "        A -- the output of the neural network\n",
    "        X -- input data\n",
    "        Y -- \"true\" labels vectors\n",
    "    \n",
    "    Returns:\n",
    "        grads -- pythin dice containing gradients\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    dZ = A - Y\n",
    "    dW = 1/m * np.dot(dZ, X.T)\n",
    "    db = 1/m * np.sum(dZ, axis = 1, keepdims = True)\n",
    "    \n",
    "    grads = {\"dW\" : dW,\n",
    "             \"db\" : db}\n",
    "    \n",
    "    return grads\n",
    "\n",
    "grads = backward_propagation(A, X, Y)\n",
    "\n",
    "print(\"dW = \" + str(grads[\"dW\"]))\n",
    "print(\"db = \" + str(grads[\"db\"]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Updating Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W updated = [[-0.29299405  0.0776744 ]]\n",
      "b updated = [[-0.25223054]]\n"
     ]
    }
   ],
   "source": [
    "def update_parameters(parameters, grads, learning_rate = 1.5):\n",
    "    \"\"\"Update parameters using gradient descent update rule\n",
    "\n",
    "    Args:\n",
    "        parameters -- python dictionary containing parameters \n",
    "        grads -- python dictionary containing gradients \n",
    "        learning_rate -- learning rate parameter for gradient descent\n",
    "        \n",
    "    Returns:\n",
    "        parameters -- python dictionary containing updated parameters \n",
    "    \"\"\"\n",
    "    \n",
    "    W, b = parameters[\"W\"], parameters[\"b\"]\n",
    "    dW, db = grads[\"dW\"], grads[\"db\"]\n",
    "    \n",
    "    W = W - learning_rate * dW\n",
    "    b = b - learning_rate * db\n",
    "    \n",
    "    parameters = {\"W\" : W,\n",
    "                  \"b\" : b}\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "parameters_updated = update_parameters(parameters, grads)\n",
    "\n",
    "print(\"W updated = \" + str(parameters_updated[\"W\"]))\n",
    "print(\"b updated = \" + str(parameters_updated[\"b\"]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Integrating all the helper functions in `nn_model()` to make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_model(X, Y, num_iterations, learning_rate, print_cost = False):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        X -- dataset\n",
    "        Y -- labels\n",
    "        num_iterations -- number of iterations in the loop\n",
    "        learning_rate -- learning rate parameter for gradient descent\n",
    "        print_cost -- if True, print the cost every iteration\n",
    "    \n",
    "    Returns:\n",
    "        parameters -- parameters learnt by the model. They can then be used to make predictions.\n",
    "    \"\"\"\n",
    "    n_x = layer_sizes(X, Y)[0]\n",
    "    n_y = layer_sizes(X, Y)[1]\n",
    "    parameters = initialize_parameters(n_x, n_y)\n",
    "    \n",
    "    for i in range(0, num_iterations):\n",
    "        A = forward_propagation(X, parameters)\n",
    "        cost = compute_cost(A, Y)\n",
    "        grads = backward_propagation(A, X, Y)\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "        if print_cost:\n",
    "            print(\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.694212\n",
      "Cost after iteration 1: 0.603965\n",
      "Cost after iteration 2: 0.550309\n",
      "Cost after iteration 3: 0.508346\n",
      "Cost after iteration 4: 0.472946\n",
      "Cost after iteration 5: 0.442386\n",
      "Cost after iteration 6: 0.415697\n",
      "Cost after iteration 7: 0.392180\n",
      "Cost after iteration 8: 0.371295\n",
      "Cost after iteration 9: 0.352611\n",
      "Cost after iteration 10: 0.335786\n",
      "Cost after iteration 11: 0.320545\n",
      "Cost after iteration 12: 0.306662\n",
      "Cost after iteration 13: 0.293956\n",
      "Cost after iteration 14: 0.282276\n",
      "Cost after iteration 15: 0.271495\n",
      "Cost after iteration 16: 0.261510\n",
      "Cost after iteration 17: 0.252231\n",
      "Cost after iteration 18: 0.243584\n",
      "Cost after iteration 19: 0.235503\n",
      "Cost after iteration 20: 0.227932\n",
      "Cost after iteration 21: 0.220824\n",
      "Cost after iteration 22: 0.214136\n",
      "Cost after iteration 23: 0.207831\n",
      "Cost after iteration 24: 0.201876\n",
      "Cost after iteration 25: 0.196243\n",
      "Cost after iteration 26: 0.190906\n",
      "Cost after iteration 27: 0.185842\n",
      "Cost after iteration 28: 0.181030\n",
      "Cost after iteration 29: 0.176452\n",
      "Cost after iteration 30: 0.172091\n",
      "Cost after iteration 31: 0.167932\n",
      "Cost after iteration 32: 0.163961\n",
      "Cost after iteration 33: 0.160166\n",
      "Cost after iteration 34: 0.156536\n",
      "Cost after iteration 35: 0.153060\n",
      "Cost after iteration 36: 0.149729\n",
      "Cost after iteration 37: 0.146533\n",
      "Cost after iteration 38: 0.143466\n",
      "Cost after iteration 39: 0.140518\n",
      "Cost after iteration 40: 0.137684\n",
      "Cost after iteration 41: 0.134957\n",
      "Cost after iteration 42: 0.132332\n",
      "Cost after iteration 43: 0.129802\n",
      "Cost after iteration 44: 0.127364\n",
      "Cost after iteration 45: 0.125011\n",
      "Cost after iteration 46: 0.122740\n",
      "Cost after iteration 47: 0.120546\n",
      "Cost after iteration 48: 0.118426\n",
      "Cost after iteration 49: 0.116376\n",
      "Cost after iteration 50: 0.114393\n",
      "Cost after iteration 51: 0.112474\n",
      "Cost after iteration 52: 0.110615\n",
      "Cost after iteration 53: 0.108815\n",
      "Cost after iteration 54: 0.107069\n",
      "Cost after iteration 55: 0.105376\n",
      "Cost after iteration 56: 0.103734\n",
      "Cost after iteration 57: 0.102140\n",
      "Cost after iteration 58: 0.100593\n",
      "Cost after iteration 59: 0.099090\n",
      "Cost after iteration 60: 0.097629\n",
      "Cost after iteration 61: 0.096209\n",
      "Cost after iteration 62: 0.094828\n",
      "Cost after iteration 63: 0.093485\n",
      "Cost after iteration 64: 0.092177\n",
      "Cost after iteration 65: 0.090905\n",
      "Cost after iteration 66: 0.089666\n",
      "Cost after iteration 67: 0.088458\n",
      "Cost after iteration 68: 0.087282\n",
      "Cost after iteration 69: 0.086135\n",
      "Cost after iteration 70: 0.085017\n",
      "Cost after iteration 71: 0.083927\n",
      "Cost after iteration 72: 0.082863\n",
      "Cost after iteration 73: 0.081825\n",
      "Cost after iteration 74: 0.080812\n",
      "Cost after iteration 75: 0.079822\n",
      "Cost after iteration 76: 0.078856\n",
      "Cost after iteration 77: 0.077912\n",
      "Cost after iteration 78: 0.076989\n",
      "Cost after iteration 79: 0.076088\n",
      "Cost after iteration 80: 0.075206\n",
      "Cost after iteration 81: 0.074344\n",
      "Cost after iteration 82: 0.073501\n",
      "Cost after iteration 83: 0.072676\n",
      "Cost after iteration 84: 0.071869\n",
      "Cost after iteration 85: 0.071079\n",
      "Cost after iteration 86: 0.070306\n",
      "Cost after iteration 87: 0.069549\n",
      "Cost after iteration 88: 0.068807\n",
      "Cost after iteration 89: 0.068080\n",
      "Cost after iteration 90: 0.067369\n",
      "Cost after iteration 91: 0.066671\n",
      "Cost after iteration 92: 0.065987\n",
      "Cost after iteration 93: 0.065317\n",
      "Cost after iteration 94: 0.064660\n",
      "Cost after iteration 95: 0.064015\n",
      "Cost after iteration 96: 0.063383\n",
      "Cost after iteration 97: 0.062763\n",
      "Cost after iteration 98: 0.062155\n",
      "Cost after iteration 99: 0.061557\n",
      "W = [[-5.3029567   4.99557606]]\n",
      "b = [[-2.52961934]]\n"
     ]
    }
   ],
   "source": [
    "parameters = nn_model(X, Y, num_iterations=100, learning_rate=1.5, print_cost=True)\n",
    "print(\"W = \" + str(parameters[\"W\"]))\n",
    "print(\"b = \" + str(parameters[\"b\"]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that after about $60$ iterations the cost function does keep decreasing, but not as much. It is a sign that it might be reasonable to stop training there. The final model parameters can be used to find the boundary line and for making predictions. Let's visualize the boundary line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
